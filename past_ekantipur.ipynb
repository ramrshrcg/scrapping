{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a1acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from datetime import date\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bd9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sections= ['news','business','opinion','sports','national','entertainment','photo_feature','feature'\n",
    "#            ,'world','blog','koseli','diaspora','Education']\n",
    "# # sections=['news']\n",
    "# year=2025\n",
    "# start_month=2\n",
    "# if (start_month ==12):\n",
    "#     end_month=1\n",
    "#     year+=1\n",
    "# else:\n",
    "#     end_month=start_month+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd0c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def news_exreact(links, df):\n",
    "#     for link in links:\n",
    "        \n",
    "#         response= requests.get(link)\n",
    "#         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#         heading= soup.find('h1').text.strip()\n",
    "#         heading = re.sub(r'[a-zA-Z0-9/.-]', '', heading)\n",
    "#         print(heading)\n",
    "        \n",
    "#         paragraphs_content = soup.find('div', class_='description current-news-block')\n",
    "#         try:\n",
    "#             paragraphs = paragraphs_content.find_all('p')\n",
    "\n",
    "            \n",
    "#         except:\n",
    "#                 print(\"No paragraphs found\")\n",
    "        \n",
    "#         all_paragraph = ''\n",
    "    \n",
    "#         for paragraph in paragraphs:\n",
    "#             paragraph= paragraph.text.strip()\n",
    "#             if(paragraph != '' and paragraph != ' ' and paragraph != '\\n'):\n",
    "#                 paragraph = re.sub(r'[a-zA-Z0-9/.-]', '', paragraph)\n",
    "#                 paragraph = paragraph.replace('\\u2013', '')\n",
    "#                 all_paragraph=all_paragraph+paragraph +'\\n'\n",
    "                \n",
    "#         all_paragraph ='\"'+ all_paragraph+'\"'\n",
    "#         print(all_paragraph)\n",
    "        \n",
    "#         df.loc[len(df)]=[heading, all_paragraph]\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# # Define the URL of the page to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aee6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def change_month(month):\n",
    "#     if month == 1:\n",
    "#         return 'Jan'\n",
    "#     elif month == 2:\n",
    "#         return 'Feb'\n",
    "#     elif month == 3:\n",
    "#         return 'Mar'\n",
    "#     elif month == 4:\n",
    "#         return 'Apr'\n",
    "#     elif month == 5:\n",
    "#         return 'May'\n",
    "#     elif month == 6:\n",
    "#         return 'Jun'\n",
    "#     elif month == 7:\n",
    "#         return 'Jul'\n",
    "#     elif month == 8:\n",
    "#         return 'Aug'\n",
    "#     elif month == 9:\n",
    "#         return 'Sep'\n",
    "#     elif month == 10:\n",
    "#         return 'Oct'\n",
    "#     elif month == 11:\n",
    "#         return 'Nov'\n",
    "#     else:\n",
    "#         return 'Dec'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ae6e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def start_extraction(sections, year, start_month, end_month):\n",
    "\n",
    "#     # sections=['news']\n",
    "\n",
    "#     for section in sections:\n",
    "#         # url= f'https://ekantipur.com/search/2025/{section}?date-from=2025-01-01&date-to=2025-02-01'\n",
    "#         url=f'https://ekantipur.com/search/{year}/{section}?date-from={year}-{start_month}-01&date-to={year}-{end_month}-01'\n",
    "\n",
    "\n",
    "#         response= requests.get(url)\n",
    "#         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "#         div_content = soup.find('div', class_='row')\n",
    "#         # print(div_content)\n",
    "        \n",
    "        \n",
    "#         table_content = div_content.find('div', class_='col-xs-12 col-sm-12 col-md-12')\n",
    "#         # print(table_content)\n",
    "#         # print(len(table_content))\n",
    "        \n",
    "#         articles_of_table_content=table_content.find_all('div', class_='teaser offset')\n",
    "#         # print((articles_of_table_content))\n",
    "        \n",
    "#         links=[]\n",
    "#         BASE_URL='https://ekantipur.com'\n",
    "#         for article in articles_of_table_content:\n",
    "#         # headline_Link=BASE_URL+((article.find('h2')).find('a')['href'])\n",
    "#         # link= headline.find('a')['href']\n",
    "        \n",
    "#             headline_link= article.find('a')['href']\n",
    "#             if not headline_link.startswith('http'):\n",
    "#                 headline_link = BASE_URL + headline_link\n",
    "        \n",
    "#                 links.append(headline_link)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "#         blocksLayout= div_content.find('div',class_='blocksLayout')\n",
    "#         # print(blocksLayout)\n",
    "    \n",
    "#         articles_of_blockLayout=blocksLayout.find_all(\"article\")\n",
    "#         # print((articles_of_blockLayout[0]))\n",
    "        \n",
    "#         for article in articles_of_blockLayout:\n",
    "#             headline_link = article.find('a')['href']\n",
    "#             if not headline_link.startswith('http'):\n",
    "#                 headline_link = BASE_URL + headline_link\n",
    "        \n",
    "#                 links.append(headline_link)\n",
    "            \n",
    "#             # print(headline_link)\n",
    "#             links.append(headline_link)\n",
    "            \n",
    "#         #remove repeated links\n",
    "#         links = list(set(links))\n",
    "#         # print(len(links))\n",
    "\n",
    "#         data = {\n",
    "#                 'शीर्षक':[],\n",
    "#                 'विवरण':[],\n",
    "#                 }\n",
    "\n",
    "#         df= pd.DataFrame(data)\n",
    "        \n",
    "#         # sentLink= [links[0], links[1]]\n",
    "        \n",
    "#         news_exreact(links, df)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         # today = date.today().strftime(\"%Y-%m-%d\") \n",
    "\n",
    "#         foldername='/var/home/ramrshrcg/Desktop/Python/scrapping/News_csv/ekantipur_past_news/'\n",
    "        \n",
    "#         start= change_month(start_month)\n",
    "#         end= change_month(end_month)\n",
    "\n",
    "#         file_path = f\"{foldername}{section}/{year}-{start}-to-{end} _ekantipur_news.csv\"\n",
    "#         if not os.path.exists(os.path.dirname(file_path)):\n",
    "#             os.makedirs(os.path.dirname(file_path))\n",
    "\n",
    "#         if os.path.exists(file_path):\n",
    "#                 df.to_csv(file_path, mode='a', index=False, header=False)\n",
    "#         else:\n",
    "#             df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be957eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_extraction(sections,year, start_month, end_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ade8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from datetime import date\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('news_scraper.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_session_with_retry():\n",
    "    \"\"\"Create a requests session with retry strategy\"\"\"\n",
    "    session = requests.Session()\n",
    "    retry_strategy = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    })\n",
    "    return session\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text content\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove unwanted characters but keep Nepali text\n",
    "    text = re.sub(r'[^\\u0900-\\u097F\\s\\w\\.,!?;:()\\-\\'\\\"]+', '', text)\n",
    "    text = text.replace('\\u2013', ' ')  # Replace en-dash\n",
    "    text = text.replace('\\u2014', ' ')  # Replace em-dash\n",
    "    text = re.sub(r'\\s+', ' ', text)    # Replace multiple spaces with single space\n",
    "    text = re.sub(r'[a-zA-Z0-9/.-]', '', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def news_extract(links, df, session, max_retries=3):\n",
    "    \"\"\"Extract news content from links with improved error handling\"\"\"\n",
    "    successful_extractions = 0\n",
    "    failed_extractions = 0\n",
    "    \n",
    "    for i, link in enumerate(links, 1):\n",
    "        logger.info(f\"Processing article {i}/{len(links)}: {link}\")\n",
    "        \n",
    "        retry_count = 0\n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                # Add delay to be respectful to the server\n",
    "                time.sleep(1)\n",
    "                \n",
    "                response = session.get(link, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "                # Extract heading with better error handling\n",
    "                heading_element = soup.find('h1')\n",
    "                if heading_element:\n",
    "                    heading = clean_text(heading_element.get_text())\n",
    "                else:\n",
    "                    logger.warning(f\"No heading found for {link}\")\n",
    "                    heading = \"शीर्षक उपलब्ध छैन\"\n",
    "\n",
    "                # Try multiple selectors for content\n",
    "                content_selectors = [\n",
    "                    'div.description.current-news-block',\n",
    "                    'div.description',\n",
    "                    'div.news-content',\n",
    "                    'div.content',\n",
    "                    'article'\n",
    "                ]\n",
    "                \n",
    "                paragraphs = []\n",
    "                paragraphs_content = None\n",
    "                \n",
    "                for selector in content_selectors:\n",
    "                    paragraphs_content = soup.select_one(selector)\n",
    "                    if paragraphs_content:\n",
    "                        paragraphs = paragraphs_content.find_all('p')\n",
    "                        if paragraphs:\n",
    "                            break\n",
    "                \n",
    "                if not paragraphs:\n",
    "                    logger.warning(f\"No paragraphs found for {link}\")\n",
    "                    all_paragraph = \"सामग्री उपलब्ध छैन\"\n",
    "                else:\n",
    "                    paragraph_texts = []\n",
    "                    for paragraph in paragraphs:\n",
    "                        paragraph_text = clean_text(paragraph.get_text())\n",
    "                        if paragraph_text and len(paragraph_text.strip()) > 0:\n",
    "                            paragraph_texts.append(paragraph_text)\n",
    "                    \n",
    "                    all_paragraph = '\\n'.join(paragraph_texts) if paragraph_texts else \"सामग्री उपलब्ध छैन\"\n",
    "                \n",
    "                # Add to dataframe\n",
    "                new_row = pd.DataFrame({\n",
    "                    'शीर्षक': [heading],\n",
    "                    'विवरण': [all_paragraph],\n",
    "                   \n",
    "                })\n",
    "                df = pd.concat([df, new_row], ignore_index=True)\n",
    "                \n",
    "                successful_extractions += 1\n",
    "                logger.info(f\"Successfully extracted: {heading[:50]}...\")\n",
    "                break  # Success, exit retry loop\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                retry_count += 1\n",
    "                logger.error(f\"Request error for {link} (attempt {retry_count}/{max_retries}): {str(e)}\")\n",
    "                if retry_count < max_retries:\n",
    "                    time.sleep(2 ** retry_count)  # Exponential backoff\n",
    "                else:\n",
    "                    failed_extractions += 1\n",
    "                    logger.error(f\"Failed to extract after {max_retries} attempts: {link}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                logger.error(f\"Unexpected error for {link} (attempt {retry_count}/{max_retries}): {str(e)}\")\n",
    "                if retry_count >= max_retries:\n",
    "                    failed_extractions += 1\n",
    "    \n",
    "    logger.info(f\"Extraction complete. Success: {successful_extractions}, Failed: {failed_extractions}\")\n",
    "    return df\n",
    "\n",
    "def validate_inputs(sections, year, start_month, end_month):\n",
    "    \"\"\"Validate input parameters\"\"\"\n",
    "    if not sections or not isinstance(sections, list):\n",
    "        raise ValueError(\"Sections must be a non-empty list\")\n",
    "    \n",
    "    if not isinstance(year, int) or year < 2000 or year > 2030:\n",
    "        raise ValueError(\"Year must be a valid integer between 2000 and 2030\")\n",
    "    \n",
    "    if not isinstance(start_month, int) or start_month < 1 or start_month > 12:\n",
    "        raise ValueError(\"Start month must be between 1 and 12\")\n",
    "    \n",
    "    if not isinstance(end_month, int) or end_month < 1 or end_month > 12:\n",
    "        raise ValueError(\"End month must be between 1 and 12\")\n",
    "    \n",
    "    if start_month > end_month:\n",
    "        raise ValueError(\"Start month cannot be greater than end month\")\n",
    "\n",
    "def change_month(month_num):\n",
    "    \"\"\"Convert month number to month name (assuming this function exists)\"\"\"\n",
    "    months = {\n",
    "        1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr',\n",
    "        5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug',\n",
    "        9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'\n",
    "    }\n",
    "    return months.get(month_num, str(month_num).zfill(2))\n",
    "\n",
    "def extract_links_from_page(soup, base_url):\n",
    "    \"\"\"Extract article links from the search results page\"\"\"\n",
    "    links = []\n",
    "    \n",
    "    try:\n",
    "        # Find the main content div\n",
    "        div_content = soup.find('div', class_='row')\n",
    "        if not div_content:\n",
    "            logger.warning(\"Main content div not found\")\n",
    "            return links\n",
    "        \n",
    "        # Extract links from table content\n",
    "        table_content = div_content.find('div', class_='col-xs-12 col-sm-12 col-md-12')\n",
    "        if table_content:\n",
    "            articles_of_table_content = table_content.find_all('div', class_='teaser offset')\n",
    "            for article in articles_of_table_content:\n",
    "                try:\n",
    "                    link_element = article.find('a')\n",
    "                    if link_element and link_element.get('href'):\n",
    "                        headline_link = urljoin(base_url, link_element['href'])\n",
    "                        links.append(headline_link)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error extracting link from table content: {str(e)}\")\n",
    "        \n",
    "        # Extract links from blocks layout\n",
    "        blocks_layout = div_content.find('div', class_='blocksLayout')\n",
    "        if blocks_layout:\n",
    "            articles_of_block_layout = blocks_layout.find_all(\"article\")\n",
    "            for article in articles_of_block_layout:\n",
    "                try:\n",
    "                    link_element = article.find('a')\n",
    "                    if link_element and link_element.get('href'):\n",
    "                        headline_link = urljoin(base_url, link_element['href'])\n",
    "                        links.append(headline_link)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error extracting link from blocks layout: {str(e)}\")\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        links = list(dict.fromkeys(links))\n",
    "        logger.info(f\"Found {len(links)} unique article links\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting links from page: {str(e)}\")\n",
    "    \n",
    "    return links\n",
    "\n",
    "def start_extraction(sections, year, start_month, end_month, base_folder='/var/home/ramrshrcg/Desktop/Python/scrapping/News_csv/ekantipur_past_news/'):\n",
    "    \"\"\"Main extraction function with comprehensive error handling\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Validate inputs\n",
    "        validate_inputs(sections, year, start_month, end_month)\n",
    "        \n",
    "        # Create session with retry strategy\n",
    "        session = create_session_with_retry()\n",
    "        base_url = 'https://ekantipur.com'\n",
    "        \n",
    "        logger.info(f\"Starting extraction for {len(sections)} sections from {year}-{start_month:02d} to {year}-{end_month:02d}\")\n",
    "        \n",
    "        for section in sections:\n",
    "            logger.info(f\"Processing section: {section}\")\n",
    "            \n",
    "            try:\n",
    "                # Construct URL\n",
    "                url = f'{base_url}/search/{year}/{section}?date-from={year}-{start_month:02d}-01&date-to={year}-{end_month:02d}-01'\n",
    "                logger.info(f\"Scraping URL: {url}\")\n",
    "                \n",
    "                # Get the search results page\n",
    "                response = session.get(url, timeout=15)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Extract article links\n",
    "                links = extract_links_from_page(soup, base_url)\n",
    "                \n",
    "                if not links:\n",
    "                    logger.warning(f\"No links found for section {section}\")\n",
    "                    continue\n",
    "                \n",
    "                # Initialize dataframe\n",
    "                df = pd.DataFrame({\n",
    "                    'शीर्षक': [],\n",
    "                    'विवरण': [],\n",
    "                    # 'लिंक': []\n",
    "                })\n",
    "                \n",
    "                # Extract news content\n",
    "                df = news_extract(links, df, session)\n",
    "                \n",
    "                if df.empty:\n",
    "                    logger.warning(f\"No data extracted for section {section}\")\n",
    "                    continue\n",
    "                \n",
    "                # Prepare file path\n",
    "                start_month_name = change_month(start_month)\n",
    "                end_month_name = change_month(end_month)\n",
    "                \n",
    "                file_path = os.path.join(\n",
    "                    base_folder,\n",
    "            \n",
    "                    section,\n",
    "                    f\"{year}-{start_month_name}-to-{end_month_name}_ekantipur_news.csv\"\n",
    "                )\n",
    "                \n",
    "                # Create directory if it doesn't exist\n",
    "                os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "                \n",
    "                # Save to CSV\n",
    "                if os.path.exists(file_path):\n",
    "                    df.to_csv(file_path, mode='a', index=False, header=False, encoding='utf-8')\n",
    "                    logger.info(f\"Appended {len(df)} articles to existing file: {file_path}\")\n",
    "                else:\n",
    "                    df.to_csv(file_path, index=False, encoding='utf-8')\n",
    "                    logger.info(f\"Created new file with {len(df)} articles: {file_path}\")\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.error(f\"Network error for section {section}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing section {section}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "            # Add delay between sections\n",
    "            time.sleep(2)\n",
    "        \n",
    "        logger.info(\"Extraction process completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error in start_extraction: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        sections= ['news','business','opinion','sports','national','entertainment','photo_feature','feature'\n",
    "           ,'world','blog','koseli','diaspora','Education'] # Add more sections as needed\n",
    "        start_extraction(sections, 2025, 1,6)\n",
    "        # url='https://ekantipur.com/search/2014/news?date-from=2014-01-01&date-to=2014-12-31'\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Script execution failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2407178a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

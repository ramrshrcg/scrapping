{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1202c3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF splitting functions ready!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "import math\n",
    "\n",
    "def split_pdf_by_size(input_path, max_size_mb=2, output_dir=None):\n",
    "    \"\"\"\n",
    "    Split a PDF into smaller chunks based on file size\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to the input PDF file\n",
    "        max_size_mb: Maximum size per chunk in MB (default: 2MB for optimal speed)\n",
    "        output_dir: Directory to save chunks (default: same as input file)\n",
    "    \n",
    "    Returns:\n",
    "        List of paths to the created PDF chunks\n",
    "    \"\"\"\n",
    "    # Get input file info\n",
    "    input_size_mb = os.path.getsize(input_path) / (1024 * 1024)\n",
    "    filename = os.path.basename(input_path)\n",
    "    name_without_ext = os.path.splitext(filename)[0]\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(input_path)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    chunks_dir = os.path.join(output_dir, f\"{name_without_ext}_chunks\")\n",
    "    if not os.path.exists(chunks_dir):\n",
    "        os.makedirs(chunks_dir)\n",
    "    \n",
    "    print(f\"Original PDF size: {input_size_mb:.2f} MB\")\n",
    "    print(f\"Target chunk size: {max_size_mb} MB\")\n",
    "    \n",
    "    # If file is already small enough, just copy it\n",
    "    if input_size_mb <= max_size_mb:\n",
    "        print(\"PDF is already small enough, no splitting needed\")\n",
    "        chunk_path = os.path.join(chunks_dir, f\"{name_without_ext}_chunk_001.pdf\")\n",
    "        import shutil\n",
    "        shutil.copy2(input_path, chunk_path)\n",
    "        return [chunk_path]\n",
    "    \n",
    "    # Read the PDF\n",
    "    reader = PdfReader(input_path)\n",
    "    total_pages = len(reader.pages)\n",
    "    \n",
    "    # Estimate pages per chunk with maximum limit\n",
    "    estimated_chunks = math.ceil(input_size_mb / max_size_mb)\n",
    "    pages_per_chunk = max(1, total_pages // estimated_chunks)\n",
    "    \n",
    "    # Enforce maximum of 35 pages per chunk\n",
    "    if pages_per_chunk > 35:\n",
    "        pages_per_chunk = 35\n",
    "        # Recalculate chunks based on page limit\n",
    "        estimated_chunks = math.ceil(total_pages / pages_per_chunk)\n",
    "    \n",
    "    print(f\"Total pages: {total_pages}\")\n",
    "    print(f\"Estimated chunks: {estimated_chunks}\")\n",
    "    print(f\"Pages per chunk: {pages_per_chunk}\")\n",
    "    \n",
    "    chunk_paths = []\n",
    "    chunk_num = 1\n",
    "    \n",
    "    for start_page in range(0, total_pages, pages_per_chunk):\n",
    "        end_page = min(start_page + pages_per_chunk, total_pages)\n",
    "        \n",
    "        # Create a new PDF writer for this chunk\n",
    "        writer = PdfWriter()\n",
    "        \n",
    "        # Add pages to this chunk\n",
    "        for page_num in range(start_page, end_page):\n",
    "            writer.add_page(reader.pages[page_num])\n",
    "        \n",
    "        # Save the chunk\n",
    "        chunk_filename = f\"{name_without_ext}_chunk_{chunk_num:03d}.pdf\"\n",
    "        chunk_path = os.path.join(chunks_dir, chunk_filename)\n",
    "        \n",
    "        with open(chunk_path, 'wb') as output_file:\n",
    "            writer.write(output_file)\n",
    "        \n",
    "        chunk_size_mb = os.path.getsize(chunk_path) / (1024 * 1024)\n",
    "        print(f\"Created chunk {chunk_num}: {chunk_filename} ({chunk_size_mb:.2f} MB, pages {start_page+1}-{end_page})\")\n",
    "        \n",
    "        chunk_paths.append(chunk_path)\n",
    "        chunk_num += 1\n",
    "    \n",
    "    print(f\"\\nPDF split into {len(chunk_paths)} chunks\")\n",
    "    print(f\"Chunks saved in: {chunks_dir}\")\n",
    "    \n",
    "    return chunk_paths\n",
    "\n",
    "def split_pdf_by_pages(input_path, pages_per_chunk=35, output_dir=None):\n",
    "    \"\"\"\n",
    "    Split a PDF into smaller chunks based on number of pages\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to the input PDF file\n",
    "        pages_per_chunk: Number of pages per chunk (default: 35, max recommended)\n",
    "        output_dir: Directory to save chunks (default: same as input file)\n",
    "    \n",
    "    Returns:\n",
    "        List of paths to the created PDF chunks\n",
    "    \"\"\"\n",
    "    # Get input file info\n",
    "    filename = os.path.basename(input_path)\n",
    "    name_without_ext = os.path.splitext(filename)[0]\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(input_path)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    chunks_dir = os.path.join(output_dir, f\"{name_without_ext}_chunks\")\n",
    "    if not os.path.exists(chunks_dir):\n",
    "        os.makedirs(chunks_dir)\n",
    "    \n",
    "    # Read the PDF\n",
    "    reader = PdfReader(input_path)\n",
    "    total_pages = len(reader.pages)\n",
    "    \n",
    "    print(f\"Total pages: {total_pages}\")\n",
    "    print(f\"Pages per chunk: {pages_per_chunk}\")\n",
    "    \n",
    "    chunk_paths = []\n",
    "    chunk_num = 1\n",
    "    \n",
    "    for start_page in range(0, total_pages, pages_per_chunk):\n",
    "        end_page = min(start_page + pages_per_chunk, total_pages)\n",
    "        \n",
    "        # Create a new PDF writer for this chunk\n",
    "        writer = PdfWriter()\n",
    "        \n",
    "        # Add pages to this chunk\n",
    "        for page_num in range(start_page, end_page):\n",
    "            writer.add_page(reader.pages[page_num])\n",
    "        \n",
    "        # Save the chunk\n",
    "        chunk_filename = f\"{name_without_ext}_chunk_{chunk_num:03d}.pdf\"\n",
    "        chunk_path = os.path.join(chunks_dir, chunk_filename)\n",
    "        \n",
    "        with open(chunk_path, 'wb') as output_file:\n",
    "            writer.write(output_file)\n",
    "        \n",
    "        chunk_size_mb = os.path.getsize(chunk_path) / (1024 * 1024)\n",
    "        print(f\"Created chunk {chunk_num}: {chunk_filename} ({chunk_size_mb:.2f} MB, pages {start_page+1}-{end_page})\")\n",
    "        \n",
    "        chunk_paths.append(chunk_path)\n",
    "        chunk_num += 1\n",
    "    \n",
    "    print(f\"\\nPDF split into {len(chunk_paths)} chunks\")\n",
    "    print(f\"Chunks saved in: {chunks_dir}\")\n",
    "    \n",
    "    return chunk_paths\n",
    "\n",
    "# Example usage:\n",
    "# For size-based splitting (recommended for OCR website limits)\n",
    "# chunk_paths = split_pdf_by_size(\"/path/to/large_file.pdf\", max_size_mb=5)\n",
    "\n",
    "# For page-based splitting\n",
    "# chunk_paths = split_pdf_by_pages(\"/path/to/large_file.pdf\", pages_per_chunk=10)\n",
    "\n",
    "print(\"PDF splitting functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81102a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processing functions ready! üöÄ\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import shutil\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pyperclip\n",
    "from module.playsound import play_sound\n",
    "\n",
    "\n",
    "def process_pdf_chunk_through_ocr(driver, chunk_path, output_base_dir):\n",
    "    \"\"\"\n",
    "    Process a single PDF chunk through the OCR workflow - COMPLETE PROCESSING\n",
    "    \n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance\n",
    "        chunk_path: Path to the PDF chunk to process\n",
    "        output_base_dir: Base directory for output files\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with processing results\n",
    "    \"\"\"\n",
    "    chunk_filename = os.path.basename(chunk_path)\n",
    "    chunk_name = os.path.splitext(chunk_filename)[0]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {chunk_filename}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # STEP 1: Navigate to fresh OCR page\n",
    "        print(\"üåê Step 1: Loading fresh OCR page...\")\n",
    "        driver.get('https://www.i2ocr.com/free-online-nepali-ocr')\n",
    "        time.sleep(5)  # Wait for page to load completely\n",
    "        \n",
    "        # STEP 2: Upload the chunk file\n",
    "        print(\"üìÑ Step 2: Uploading PDF chunk...\")\n",
    "        button_for_image_upload = driver.find_element(By.ID, 'i2ocr_uploadedfile')\n",
    "        button_for_image_upload.send_keys(chunk_path)\n",
    "        print(\"‚úì File uploaded successfully\")\n",
    "        time.sleep(3)  # Wait for file to upload\n",
    "        \n",
    "        # STEP 3: Handle initial cookies\n",
    "        print(\"üç™ Step 3: Handling cookies...\")\n",
    "        try:\n",
    "            time.sleep(2)\n",
    "            cookie_selectors = [\n",
    "                'button[id*=\"accept\"]',\n",
    "                'button[class*=\"accept\"]',\n",
    "                'button[id*=\"cookie\"]',\n",
    "                'button[class*=\"cookie\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in cookie_selectors:\n",
    "                try:\n",
    "                    cookie_button = driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                    if cookie_button.is_displayed():\n",
    "                        cookie_button.click()\n",
    "                        print(\"‚úì Initial cookies accepted\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Cookie handling: {e}\")\n",
    "        \n",
    "        # STEP 4: Handle reCAPTCHA with manual verification for complex CAPTCHAs\n",
    "        print(\"üîê Step 4: Handling reCAPTCHA...\")\n",
    "        captcha_solved = False\n",
    "        \n",
    "        try:\n",
    "            # Look for reCAPTCHA iframe\n",
    "            recaptcha_iframe = None\n",
    "            iframe_selectors = [\n",
    "                'iframe[src*=\"recaptcha\"]',\n",
    "                'iframe[title*=\"reCAPTCHA\"]',\n",
    "                'iframe[name*=\"recaptcha\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in iframe_selectors:\n",
    "                try:\n",
    "                    recaptcha_iframe = driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if recaptcha_iframe:\n",
    "                driver.switch_to.frame(recaptcha_iframe)\n",
    "                \n",
    "                # Click the checkbox\n",
    "                checkbox_selectors = [\n",
    "                    '.recaptcha-checkbox-border',\n",
    "                    '.recaptcha-checkbox',\n",
    "                    '#recaptcha-anchor'\n",
    "                ]\n",
    "                \n",
    "                for selector in checkbox_selectors:\n",
    "                    try:\n",
    "                        recaptcha_checkbox = driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                        recaptcha_checkbox.click()\n",
    "                        print(\"‚úì reCAPTCHA checkbox clicked\")\n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                driver.switch_to.default_content()\n",
    "                time.sleep(5)  # Wait for CAPTCHA processing\n",
    "                \n",
    "                # Check for complex CAPTCHA\n",
    "                complex_captcha_detected = False\n",
    "                try:\n",
    "                    complex_captcha_selectors = [\n",
    "                        'iframe[src*=\"bframe\"]',\n",
    "                        '.rc-imageselect'\n",
    "                    ]\n",
    "                    \n",
    "                    for selector in complex_captcha_selectors:\n",
    "                        try:\n",
    "                            element = driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                            if element.is_displayed():\n",
    "                                complex_captcha_detected = True\n",
    "                                break\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    if not complex_captcha_detected:\n",
    "                        page_text = driver.page_source.lower()\n",
    "                        if any(phrase in page_text for phrase in ['select all images', 'crosswalks', 'traffic lights']):\n",
    "                            complex_captcha_detected = True\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Error checking for complex CAPTCHA: {e}\")\n",
    "                \n",
    "                if complex_captcha_detected:\n",
    "                    print(\"ü§ñ Complex image CAPTCHA detected!\")\n",
    "                    print(\"üë§ Please solve the CAPTCHA manually in the browser window.\")\n",
    "                    print(\"üîç Look for image selection challenges (crosswalks, traffic lights, etc.)\")\n",
    "                    play_sound('/var/home/ramrshrcg/Music/Happy Sound Effects [kgS40C2VBBw].mp3')\n",
    "                    \n",
    "                   \n",
    "                    # Wait for user to solve CAPTCHA manually\n",
    "                    input(\"‚úã Press Enter after you have completed the CAPTCHA manually...\")\n",
    "                    print(\"‚úì Manual CAPTCHA verification completed\")\n",
    "                    captcha_solved = True\n",
    "                else:\n",
    "                    print(\"‚úì Simple reCAPTCHA completed\")\n",
    "                    captcha_solved = True\n",
    "            else:\n",
    "                print(\"‚úì No reCAPTCHA found\")\n",
    "                captcha_solved = True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error handling reCAPTCHA: {e}\")\n",
    "            print(\"‚ö†Ô∏è If there's a CAPTCHA visible, please solve it manually.\")\n",
    "            user_input = input(\"Type 'done' when CAPTCHA is solved, or 'skip' to skip this chunk: \").lower()\n",
    "            if user_input == 'done':\n",
    "                captcha_solved = True\n",
    "            else:\n",
    "                return {\"success\": False, \"error\": \"CAPTCHA solving skipped by user\"}\n",
    "        \n",
    "        if not captcha_solved:\n",
    "            print(\"‚ùå CAPTCHA not solved\")\n",
    "            return {\"success\": False, \"error\": \"CAPTCHA solving failed\"}\n",
    "        \n",
    "        # STEP 5: Handle post-reCAPTCHA cookies\n",
    "        print(\"üç™ Step 5: Handling post-reCAPTCHA cookies...\")\n",
    "        try:\n",
    "            time.sleep(5)\n",
    "            cookie_selectors = [\n",
    "                '.cc-btn.cc-dismiss',\n",
    "                'a[class*=\"cc-dismiss\"]',\n",
    "                '.cc-compliance a'\n",
    "            ]\n",
    "            \n",
    "            for selector in cookie_selectors:\n",
    "                try:\n",
    "                    cookie_button = driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                    if cookie_button.is_displayed():\n",
    "                        cookie_button.click()\n",
    "                        print(\"‚úì Post-reCAPTCHA cookies handled\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Post-reCAPTCHA cookie handling: {e}\")\n",
    "        \n",
    "        # STEP 6: Click Extract Text button and wait for processing\n",
    "        print(\"‚ö° Step 6: Starting OCR processing...\")\n",
    "        try:\n",
    "            time.sleep(3)\n",
    "            extract_button = driver.find_element(By.ID, 'submit_i2ocr')\n",
    "            extract_button.click()\n",
    "            print(\"‚úì Extract Text button clicked\")\n",
    "            \n",
    "            # Wait for OCR processing to complete\n",
    "            print(\"‚è≥ Waiting for OCR processing to complete...\")\n",
    "            time.sleep(20)  # Give more time for processing\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error clicking Extract Text button: {e}\")\n",
    "            return {\"success\": False, \"error\": \"Extract button click failed\"}\n",
    "        \n",
    "        # STEP 7: Wait for page viewer to load and extract ALL pages\n",
    "        print(\"üìñ Step 7: Processing all pages in chunk...\")\n",
    "        \n",
    "        # Create output folder for this chunk\n",
    "        chunk_output_dir = os.path.join(output_base_dir, f\"extracted_text_{chunk_name}\")\n",
    "        if not os.path.exists(chunk_output_dir):\n",
    "            os.makedirs(chunk_output_dir)\n",
    "        \n",
    "        # Wait for page viewer to load\n",
    "        max_wait = 60\n",
    "        waited = 0\n",
    "        while waited < max_wait:\n",
    "            try:\n",
    "                page_elements = driver.find_elements(By.CSS_SELECTOR, 'img[id^=\"page_\"]')\n",
    "                if page_elements:\n",
    "                    break\n",
    "                time.sleep(5)\n",
    "                waited += 5\n",
    "                print(f\"‚è≥ Still waiting for page viewer... ({waited}s)\")\n",
    "            except:\n",
    "                time.sleep(5)\n",
    "                waited += 5\n",
    "        \n",
    "        # Find all pages in the chunk\n",
    "        try:\n",
    "            page_elements = driver.find_elements(By.CSS_SELECTOR, 'img[id^=\"page_\"]')\n",
    "            total_pages = len(page_elements)\n",
    "            print(f\"üìÑ Found {total_pages} pages in chunk\")\n",
    "            \n",
    "            if total_pages == 0:\n",
    "                print(\"‚ùå No pages found in chunk\")\n",
    "                return {\"success\": False, \"error\": \"No pages found\"}\n",
    "            \n",
    "            extracted_pages = 0\n",
    "            error_page=[]\n",
    "            \n",
    "            # Process each page completely\n",
    "            for page_num in range(1, total_pages + 1):\n",
    "                try:\n",
    "                    print(f\"\\n--- Processing page {page_num}/{total_pages} ---\")\n",
    "                    \n",
    "                    # Click on page to select it\n",
    "                    page_element = driver.find_element(By.ID, f'page_{page_num}')\n",
    "                    page_element.click()\n",
    "                    print(f\"‚úì Selected page {page_num}\")\n",
    "                    time.sleep(10)\n",
    "                    \n",
    "                    # Click Extract Page Text button, retry if not found\n",
    "                    max_retries = 3\n",
    "                    for attempt in range(1, max_retries + 1):\n",
    "                        try:\n",
    "                            extract_page_button = driver.find_element(\n",
    "                                By.XPATH,\n",
    "                                \"//button[contains(text(), 'Extract Page Text')] | //input[contains(@value, 'Extract Page Text')]\"\n",
    "                            )\n",
    "                            extract_page_button.click()\n",
    "                            print(f\"‚úì Extract Page Text clicked for page {page_num} (attempt {attempt})\")\n",
    "                            time.sleep(15)  # Wait for page processing\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            print(f\"‚ö† Extract Page Text button not found for page {page_num} (attempt {attempt})\")\n",
    "                            if attempt == max_retries:\n",
    "                                error_page.append(page_num)\n",
    "                                continue\n",
    "                            else:\n",
    "                                time.sleep(5)\n",
    "                    else:\n",
    "                        # If all retries failed, skip to next page\n",
    "                        continue\n",
    "                    \n",
    "                    # Wait for this page's OCR to complete\n",
    "                    print(f\"‚è≥ Waiting for page {page_num} OCR to complete...\")\n",
    "                    max_page_wait = 60\n",
    "                    page_waited = 0\n",
    "                    while page_waited < max_page_wait:\n",
    "                        try:\n",
    "                            # Check if ocrTextBox has content\n",
    "                            ocr_textbox = driver.find_element(By.ID, 'ocrTextBox')\n",
    "                            text_content = ocr_textbox.get_attribute('value') or ocr_textbox.text\n",
    "                            if text_content and text_content.strip():\n",
    "                                print(f\"‚úì Page {page_num} OCR completed\")\n",
    "                                break\n",
    "                            else:\n",
    "                                time.sleep(5)\n",
    "                                page_waited += 5\n",
    "                                print(f\"‚è≥ Still processing page {page_num}... ({page_waited}s)\")\n",
    "                        except:\n",
    "                            time.sleep(5)\n",
    "                            page_waited += 5\n",
    "                    \n",
    "                    # Extract text from this page\n",
    "                    extracted_text = None\n",
    "                    \n",
    "                    # Method 1: Try ocrTextBox first\n",
    "                    try:\n",
    "                        ocr_textbox = driver.find_element(By.ID, 'ocrTextBox')\n",
    "                        extracted_text = ocr_textbox.get_attribute('value') or ocr_textbox.text\n",
    "                        if extracted_text and extracted_text.strip():\n",
    "                            print(f\"‚úì Text extracted from ocrTextBox\")\n",
    "                        else:\n",
    "                            extracted_text = None\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    # Method 2: Try copy button if ocrTextBox fails\n",
    "                    if not extracted_text:\n",
    "                        try:\n",
    "                            copy_button = driver.find_element(By.CSS_SELECTOR, \"button.copy_btn\")\n",
    "                            copy_button.click()\n",
    "                            time.sleep(2)\n",
    "                            pyperclip.copy('') # Clear clipboard first\n",
    "                            copy_button.click() # Click again to copy text\n",
    "                            time.sleep(2)\n",
    "                            extracted_text = pyperclip.paste()\n",
    "                            if extracted_text and extracted_text.strip():\n",
    "                                print(f\"‚úì Text extracted from clipboard\")\n",
    "                            else:\n",
    "                                extracted_text = None\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    # Save extracted text for this page\n",
    "                    if extracted_text and extracted_text.strip():\n",
    "                        page_filename = f\"page_{page_num:03d}.txt\"\n",
    "                        page_filepath = os.path.join(chunk_output_dir, page_filename)\n",
    "                        \n",
    "                        with open(page_filepath, 'w', encoding='utf-8') as f:\n",
    "                            f.write(f\"Chunk: {chunk_name}\\n\")\n",
    "                            f.write(f\"Page {page_num} Text:\\n\")\n",
    "                            f.write(\"=\"*50 + \"\\n\")\n",
    "                            f.write(extracted_text)\n",
    "                            f.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "                        \n",
    "                        print(f\"‚úÖ Page {page_num} saved: {page_filename}\")\n",
    "                        print(f\"Preview: {extracted_text[:50]}...\")\n",
    "                        extracted_pages += 1\n",
    "                    else:\n",
    "                        print(f\"‚ö† No text extracted for page {page_num}\")\n",
    "                        \n",
    "                        # Create empty file marker\n",
    "                        page_filename = f\"page_{page_num:03d}_empty.txt\"\n",
    "                        page_filepath = os.path.join(chunk_output_dir, page_filename)\n",
    "                        \n",
    "                        with open(page_filepath, 'w', encoding='utf-8') as f:\n",
    "                            f.write(f\"Chunk: {chunk_name}\\n\")\n",
    "                            f.write(f\"Page {page_num} - No text extracted\\n\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error processing page {page_num}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Create chunk summary\n",
    "            summary_path = os.path.join(chunk_output_dir, \"chunk_summary.txt\")\n",
    "            with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"Chunk Processing Summary\\n\")\n",
    "                f.write(f\"=\" * 50 + \"\\n\")\n",
    "                f.write(f\"Chunk File: {chunk_filename}\\n\")\n",
    "                f.write(f\"Total Pages: {total_pages}\\n\")\n",
    "                f.write(f\"Pages with Text: {extracted_pages}\\n\")\n",
    "                f.write(f\"Processing Time: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "                f.write(f'failed pages: {error_page}\\n')\n",
    "            \n",
    "            print(f\"‚úÖ Chunk {chunk_name} COMPLETELY processed: {extracted_pages}/{total_pages} pages\")\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"chunk_name\": chunk_name,\n",
    "                \"total_pages\": total_pages,\n",
    "                \"extracted_pages\": extracted_pages,\n",
    "                \"output_dir\": chunk_output_dir\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in page processing: {e}\")\n",
    "            return {\"success\": False, \"error\": str(e)}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing chunk: {e}\")\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "def batch_process_pdf_chunks(input_pdf_path, max_size_mb=2, pages_per_chunk=35):\n",
    "    \"\"\"\n",
    "    Complete workflow: Split PDF and process each chunk through OCR\n",
    "    \n",
    "    Args:\n",
    "        input_pdf_path: Path to the large PDF file\n",
    "        max_size_mb: Maximum size per chunk in MB (default: 2MB for optimal speed)\n",
    "        pages_per_chunk: Alternative splitting method by pages (default: 35 pages max)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with batch processing results\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ Starting batch processing for: {input_pdf_path}\")\n",
    "    \n",
    "    # Create main output directory\n",
    "    pdf_name = os.path.splitext(os.path.basename(input_pdf_path))[0]\n",
    "    output_base_dir = os.path.join(os.path.dirname(input_pdf_path), f\"{pdf_name}_batch_ocr\")\n",
    "    \n",
    "    if not os.path.exists(output_base_dir):\n",
    "        os.makedirs(output_base_dir)\n",
    "    \n",
    "    # Step 1: Split PDF\n",
    "    print(\"\\nüìÑ Step 1: Splitting PDF...\")\n",
    "    if pages_per_chunk:\n",
    "        chunk_paths = split_pdf_by_pages(input_pdf_path, pages_per_chunk, output_base_dir)\n",
    "    else:\n",
    "        # Use default 35 pages if no specific method chosen\n",
    "        default_pages = 35  # Optimal for 2-3 MB chunks\n",
    "        chunk_paths = split_pdf_by_pages(input_pdf_path, default_pages, output_base_dir)\n",
    "        \n",
    "        # Also try size-based splitting and use whichever creates smaller chunks\n",
    "        # size_chunks = split_pdf_by_size(input_pdf_path, max_size_mb, output_base_dir)\n",
    "        \n",
    "        # Use the method that creates more chunks (smaller chunks)\n",
    "        # if len(size_chunks) > len(chunk_paths):\n",
    "        #     chunk_paths = size_chunks\n",
    "        #     print(f\"Using size-based splitting ({max_size_mb}MB chunks) for better performance\")\n",
    "        # else:\n",
    "        #     print(f\"Using page-based splitting ({default_pages} pages) for better performance\")\n",
    "    \n",
    "    print(f\"‚úÖ PDF split into {len(chunk_paths)} chunks\")\n",
    "    \n",
    "    # Step 2: Initialize browser\n",
    "    print(\"\\nüåê Step 2: Initializing browser...\")\n",
    "    driver = webdriver.Firefox()\n",
    "    \n",
    "    # Step 3: Process each chunk\n",
    "    print(\"\\nüîÑ Step 3: Processing chunks through OCR...\")\n",
    "    results = []\n",
    "    total_pages_processed = 0\n",
    "    successful_chunks = 0\n",
    "    \n",
    "    try:\n",
    "        for i, chunk_path in enumerate(chunk_paths, 1):\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"üîÑ PROCESSING CHUNK {i}/{len(chunk_paths)}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            # Process this chunk completely\n",
    "            result = process_pdf_chunk_through_ocr(driver, chunk_path, output_base_dir)\n",
    "            results.append(result)\n",
    "            \n",
    "            if result[\"success\"]:\n",
    "                successful_chunks += 1\n",
    "                total_pages_processed += result.get(\"extracted_pages\", 0)\n",
    "                print(f\"\\n‚úÖ CHUNK {i} COMPLETED SUCCESSFULLY!\")\n",
    "                print(f\"   ‚Ä¢ Pages extracted: {result.get('extracted_pages', 0)}/{result.get('total_pages', 0)}\")\n",
    "                print(f\"   ‚Ä¢ Output saved to: {result.get('output_dir', 'N/A')}\")\n",
    "            else:\n",
    "                print(f\"\\n‚ùå CHUNK {i} FAILED!\")\n",
    "                print(f\"   ‚Ä¢ Error: {result.get('error', 'Unknown error')}\")\n",
    "            \n",
    "            # Wait before next chunk to ensure complete separation\n",
    "            if i < len(chunk_paths):  # Don't wait after the last chunk\n",
    "                print(f\"\\n‚è≥ Waiting 10 seconds before next chunk...\")\n",
    "                time.sleep(10)\n",
    "    \n",
    "    finally:\n",
    "        # Step 4: Close browser\n",
    "        print(\"\\nüîß Step 4: Cleaning up...\")\n",
    "        driver.quit()\n",
    "    \n",
    "    # Step 5: Create final summary\n",
    "    print(\"\\nüìä Step 5: Creating final summary...\")\n",
    "    summary_path = os.path.join(output_base_dir, \"batch_processing_summary.txt\")\n",
    "    \n",
    "    with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"Batch OCR Processing Summary\\n\")\n",
    "        f.write(f\"=\" * 60 + \"\\n\")\n",
    "        f.write(f\"Original PDF: {input_pdf_path}\\n\")\n",
    "        f.write(f\"Processing Time: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total Chunks: {len(chunk_paths)}\\n\")\n",
    "        f.write(f\"Successful Chunks: {successful_chunks}\\n\")\n",
    "        f.write(f\"Total Pages Processed: {total_pages_processed}\\n\")\n",
    "        f.write(f\"Output Directory: {output_base_dir}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Chunk Details:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            if result[\"success\"]:\n",
    "                f.write(f\"Chunk {i}: ‚úÖ {result['chunk_name']} - {result['extracted_pages']}/{result['total_pages']} pages\\n\")\n",
    "            else:\n",
    "                f.write(f\"Chunk {i}: ‚ùå Failed - {result.get('error', 'Unknown error')}\\n\")\n",
    "    \n",
    "    print(f\"\\nüéâ Batch processing complete!\")\n",
    "    print(f\"‚úÖ Successfully processed: {successful_chunks}/{len(chunk_paths)} chunks\")\n",
    "    print(f\"üìÑ Total pages extracted: {total_pages_processed}\")\n",
    "    print(f\"üìÅ Results saved in: {output_base_dir}\")\n",
    "    \n",
    "    return {\n",
    "        \"success\": True,\n",
    "        \"total_chunks\": len(chunk_paths),\n",
    "        \"successful_chunks\": successful_chunks,\n",
    "        \"total_pages\": total_pages_processed,\n",
    "        \"output_dir\": output_base_dir,\n",
    "        \"chunk_results\": results\n",
    "    }\n",
    "\n",
    "print(\"Batch processing functions ready! üöÄ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de39d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "large_pdf_path ='/var/home/ramrshrcg/Downloads/nepali datas folder/Nepali Materials/‡§™‡§∞‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§®‡•Ä‡§§‡§ø ‡•®‡•¶‡•≠‡•≠ .pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac1746c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting batch processing with size-based splitting...\n",
      "üöÄ Starting batch processing for: /var/home/ramrshrcg/Downloads/nepali datas folder/Nepali Materials/‡§™‡§∞‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§®‡•Ä‡§§‡§ø ‡•®‡•¶‡•≠‡•≠ .pdf\n",
      "\n",
      "üìÑ Step 1: Splitting PDF...\n",
      "Total pages: 31\n",
      "Pages per chunk: 35\n",
      "Created chunk 1: ‡§™‡§∞‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§®‡•Ä‡§§‡§ø ‡•®‡•¶‡•≠‡•≠ _chunk_001.pdf (0.39 MB, pages 1-31)\n",
      "\n",
      "PDF split into 1 chunks\n",
      "Chunks saved in: /var/home/ramrshrcg/Downloads/nepali datas folder/Nepali Materials/‡§™‡§∞‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§®‡•Ä‡§§‡§ø ‡•®‡•¶‡•≠‡•≠ _batch_ocr/‡§™‡§∞‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§®‡•Ä‡§§‡§ø ‡•®‡•¶‡•≠‡•≠ _chunks\n",
      "‚úÖ PDF split into 1 chunks\n",
      "\n",
      "üåê Step 2: Initializing browser...\n",
      "\n",
      "üîÑ Step 3: Processing chunks through OCR...\n",
      "\n",
      "================================================================================\n",
      "üîÑ PROCESSING CHUNK 1/1\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Processing: ‡§™‡§∞‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§®‡•Ä‡§§‡§ø ‡•®‡•¶‡•≠‡•≠ _chunk_001.pdf\n",
      "============================================================\n",
      "üåê Step 1: Loading fresh OCR page...\n",
      "üìÑ Step 2: Uploading PDF chunk...\n",
      "‚úì File uploaded successfully\n",
      "üç™ Step 3: Handling cookies...\n",
      "üîê Step 4: Handling reCAPTCHA...\n",
      "‚úì reCAPTCHA checkbox clicked\n",
      "‚úì Simple reCAPTCHA completed\n",
      "üç™ Step 5: Handling post-reCAPTCHA cookies...\n",
      "‚úì Post-reCAPTCHA cookies handled\n",
      "‚ö° Step 6: Starting OCR processing...\n",
      "‚úì Extract Text button clicked\n",
      "‚è≥ Waiting for OCR processing to complete...\n",
      "üìñ Step 7: Processing all pages in chunk...\n",
      "‚è≥ Still waiting for page viewer... (5s)\n",
      "üìÑ Found 31 pages in chunk\n",
      "\n",
      "--- Processing page 1/31 ---\n",
      "‚úì Selected page 1\n",
      "‚úì Extract Page Text clicked for page 1 (attempt 1)\n",
      "‚è≥ Waiting for page 1 OCR to complete...\n",
      "‚úì Page 1 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 1 saved: page_001.txt\n",
      "Preview: 9. ‡§™‡•É‡§∑‡•ç‡§†‡§≠‡•Ç‡§Æ‡§ø‡§É\n",
      "‡§™‡§∞‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§®‡•Ä‡§§‡§ø, ‡•®‡•¶‡•≠‡•≠\n",
      "‡§™‡§∞‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§®‡•Ä‡§§‡§ø‡§ï...\n",
      "\n",
      "--- Processing page 2/31 ---\n",
      "‚úì Selected page 2\n",
      "‚úì Extract Page Text clicked for page 2 (attempt 1)\n",
      "‚è≥ Waiting for page 2 OCR to complete...\n",
      "‚úì Page 2 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 2 saved: page_002.txt\n",
      "Preview: ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§ø‡§Ø ‡§π‡§ø‡§§ ‡§§‡§•‡§æ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ‡§≤‡§æ‡§à ‡§ï‡•á‡§®‡•ç‡§¶‡•ç‡§∞‡§¨‡§ø‡§®‡•ç‡§¶‡•Å‡§Æ‡§æ ‡§∞‡§æ‡§ñ‡•ç‡§¶...\n",
      "\n",
      "--- Processing page 3/31 ---\n",
      "‚úì Selected page 3\n",
      "‚úì Extract Page Text clicked for page 3 (attempt 1)\n",
      "‚è≥ Waiting for page 3 OCR to complete...\n",
      "‚úì Page 3 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 3 saved: page_003.txt\n",
      "Preview: ‡§∏‡§Ç‡§µ‡§ø‡§ß‡§æ‡§®‡§ï‡•ã ‡§â‡§™‡§∞‡•ç‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§®‡§ï‡§æ ‡§∏‡§æ‡§•‡•à ‡§®‡•á‡§™‡§æ‡§≤ ‡§∏‡§∞‡§ï‡§æ‡§∞‡§ï...\n",
      "\n",
      "--- Processing page 4/31 ---\n",
      "‚úì Selected page 4\n",
      "‚úì Extract Page Text clicked for page 4 (attempt 1)\n",
      "‚è≥ Waiting for page 4 OCR to complete...\n",
      "‚úì Page 4 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 4 saved: page_004.txt\n",
      "Preview: ‡•™. ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§ø‡§Ø ‡§®‡•á‡§§‡•É‡§§‡•ç‡§µ ‡§∞ ‡§∏‡•ç‡§µ‡§æ‡§Æ‡§ø‡§§‡•ç‡§µ ‡§ï‡§æ‡§Ø‡§Æ ‡§∞‡§æ‡§ñ‡•ç‡§¶‡•à ‡§∞‡§æ‡§∑‡•ç‡§ü...\n",
      "\n",
      "--- Processing page 5/31 ---\n",
      "‚úì Selected page 5\n",
      "‚úì Extract Page Text clicked for page 5 (attempt 1)\n",
      "‚è≥ Waiting for page 5 OCR to complete...\n",
      "‚úì Page 5 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 5 saved: page_005.txt\n",
      "Preview: ‡•®. ‡§Ü‡§ß‡§æ‡§∞‡§≠‡•Ç‡§§ ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§ø‡§Ø ‡§π‡§ø‡§§, ‡§Ü‡§∞‡•ç‡§•‡§ø‡§ï ‡§∏‡§Æ‡•É‡§¶‡•ç‡§ß‡§ø, ‡§∏‡•Ä‡§Æ‡§æ ‡§≤‡§ó‡§æ...\n",
      "\n",
      "--- Processing page 6/31 ---\n",
      "‚úì Selected page 6\n",
      "‚úì Extract Page Text clicked for page 6 (attempt 1)\n",
      "‚è≥ Waiting for page 6 OCR to complete...\n",
      "‚úì Page 6 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 6 saved: page_006.txt\n",
      "Preview: ‡•™.\n",
      "‡•ß‡•´. ‡§µ‡§ø‡§∂‡•ç‡§µ‡§µ‡•ç‡§Ø‡§æ‡§™‡•Ä ‡§Æ‡§π‡§æ‡§Æ‡§æ‡§∞‡•Ä ‡§™‡§õ‡§ø‡§ï‡•ã ‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§ø‡§§ ‡§™‡§∞‡§ø‡§∏‡•ç‡§•‡§ø...\n",
      "\n",
      "--- Processing page 7/31 ---\n",
      "‚úì Selected page 7\n",
      "‚úì Extract Page Text clicked for page 7 (attempt 1)\n",
      "‚è≥ Waiting for page 7 OCR to complete...\n",
      "‚úì Page 7 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 7 saved: page_007.txt\n",
      "Preview: ‡•´. ‡§∏‡§Ç‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§∏‡§Ç‡§ò‡§ï‡•ã ‡§¨‡§°‡§æ‡§™‡§§‡•ç‡§∞,\n",
      "‡•¨. ‡§®‡•á‡§™‡§æ‡§≤ ‡§™‡§ï‡•ç‡§∑ ‡§≠‡§è‡§ï...\n",
      "\n",
      "--- Processing page 8/31 ---\n",
      "‚úì Selected page 8\n",
      "‚úì Extract Page Text clicked for page 8 (attempt 1)\n",
      "‚è≥ Waiting for page 8 OCR to complete...\n",
      "‚úì Page 8 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 8 saved: page_008.txt\n",
      "Preview: ‡•ß. ‡§õ‡§ø‡§Æ‡•á‡§ï‡•Ä ‡§≤‡§ó‡§æ‡§Ø‡§§ ‡§∏‡§¨‡•à ‡§Æ‡•Å‡§≤‡•Å‡§ï‡§∏‡§Å‡§ó ‡§∏‡§æ‡§∞‡•ç‡§µ‡§≠‡•å‡§Æ‡§ø‡§ï ‡§∏‡§Æ‡§æ‡§®‡§§‡§æ, ‡§™‡§æ...\n",
      "\n",
      "--- Processing page 9/31 ---\n",
      "‚úì Selected page 9\n",
      "‚úì Extract Page Text clicked for page 9 (attempt 1)\n",
      "‚è≥ Waiting for page 9 OCR to complete...\n",
      "‚úì Page 9 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 9 saved: page_009.txt\n",
      "Preview: ‡§∞‡§£‡§®‡•Ä‡§§‡§ø ‡§§‡§•‡§æ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§®‡•Ä‡§§‡§ø\n",
      "‡•ß. ‡§õ‡§ø‡§Æ‡•á‡§ï‡•Ä ‡§Æ‡•Å‡§≤‡•Å‡§ï‡§π‡§∞‡•Ç‡§∏‡§Å‡§ó‡§ï‡•ã ‡§∏‡§Æ‡•ç‡§¨‡§®...\n",
      "\n",
      "--- Processing page 10/31 ---\n",
      "‚úì Selected page 10\n",
      "‚úì Extract Page Text clicked for page 10 (attempt 1)\n",
      "‚è≥ Waiting for page 10 OCR to complete...\n",
      "‚úì Page 10 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 10 saved: page_010.txt\n",
      "Preview: 9. ‡§õ‡§ø‡§Æ‡•á‡§ï‡•Ä ‡§Æ‡§ø‡§§‡•ç‡§∞‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§π‡§∞‡•Ç‡§ï‡•ã ‡§Ü‡§∞‡•ç‡§•‡§ø‡§ï ‡§µ‡§ø‡§ï‡§æ‡§∏ ‡§∞ ‡§∏‡§Æ‡•É‡§¶‡•ç‡§ß‡§ø...\n",
      "\n",
      "--- Processing page 11/31 ---\n",
      "‚úì Selected page 11\n",
      "‚úì Extract Page Text clicked for page 11 (attempt 1)\n",
      "‚è≥ Waiting for page 11 OCR to complete...\n",
      "‚úì Page 11 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 11 saved: page_011.txt\n",
      "Preview: ‡•≠.‡•®. ‡§®‡•Ä‡§§‡§ø\n",
      "‡§¶‡•ç‡§µ‡§ø‡§™‡§ï‡•ç‡§∑‡•Ä‡§Ø ‡§∏‡§®‡•ç‡§ß‡§ø ‡§∏‡§Æ‡•ç‡§ù‡•å‡§§‡§æ‡§≤‡§æ‡§à ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§ø‡§Ø ‡§π‡§ø...\n",
      "\n",
      "--- Processing page 12/31 ---\n",
      "‚úì Selected page 12\n",
      "‚úì Extract Page Text clicked for page 12 (attempt 1)\n",
      "‚è≥ Waiting for page 12 OCR to complete...\n",
      "‚úì Page 12 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 12 saved: page_012.txt\n",
      "Preview: ‡•≠.‡•®. ‡§®‡•Ä‡§§‡§ø\n",
      "‡§¶‡•ç‡§µ‡§ø‡§™‡§ï‡•ç‡§∑‡•Ä‡§Ø ‡§∏‡§®‡•ç‡§ß‡§ø ‡§∏‡§Æ‡•ç‡§ù‡•å‡§§‡§æ‡§≤‡§æ‡§à ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§ø‡§Ø ‡§π‡§ø...\n",
      "\n",
      "--- Processing page 13/31 ---\n",
      "‚úì Selected page 13\n",
      "‚ö† Extract Page Text button not found for page 13 (attempt 1)\n",
      "‚ö† Extract Page Text button not found for page 13 (attempt 2)\n",
      "‚ö† Extract Page Text button not found for page 13 (attempt 3)\n",
      "\n",
      "--- Processing page 14/31 ---\n",
      "‚úì Selected page 14\n",
      "‚úì Extract Page Text clicked for page 14 (attempt 1)\n",
      "‚è≥ Waiting for page 14 OCR to complete...\n",
      "‚úì Page 14 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 14 saved: page_014.txt\n",
      "Preview: ‡§∞‡§£‡§®‡•Ä‡§§‡§ø ‡§§‡§•‡§æ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§®‡•Ä‡§§‡§ø\n",
      "‡•ß. ‡§Æ‡§æ‡§®‡§µ ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞‡§ï‡•ã ‡§∏‡§Ç‡§∞‡§ï‡•ç‡§∑‡§£ ‡§∞ ‡§∏‡§Æ...\n",
      "\n",
      "--- Processing page 15/31 ---\n",
      "‚úì Selected page 15\n",
      "‚úì Extract Page Text clicked for page 15 (attempt 1)\n",
      "‚è≥ Waiting for page 15 OCR to complete...\n",
      "‚úì Page 15 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 15 saved: page_015.txt\n",
      "Preview: ‡§ï‡§æ‡§∞‡•ç‡§¨‡§® ‡§µ‡•ç‡§Ø‡§æ‡§™‡§æ‡§∞ (Carbon Trading) ‡§ï‡§æ ‡§™‡•ç‡§∞‡§æ‡§µ‡§ß‡§æ‡§®‡§≤‡§æ‡§à ‡§ï‡§æ‡§∞...\n",
      "\n",
      "--- Processing page 16/31 ---\n",
      "‚úì Selected page 16\n",
      "‚úì Extract Page Text clicked for page 16 (attempt 1)\n",
      "‚è≥ Waiting for page 16 OCR to complete...\n",
      "‚úì Page 16 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 16 saved: page_016.txt\n",
      "Preview: ‡•™. ‡§Ö‡§®‡•ç‡§§‡§∞‡•ç‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§ø‡§Ø ‡§Ü‡§™‡•ç‡§∞‡§µ‡§æ‡§∏‡§®‡§≤‡§æ‡§à ‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§ø‡§§ ‡§¨‡§®‡§æ‡§â‡§® ‡§Ö‡§®‡•ç...\n",
      "\n",
      "--- Processing page 17/31 ---\n",
      "‚úì Selected page 17\n",
      "‚úì Extract Page Text clicked for page 17 (attempt 1)\n",
      "‚è≥ Waiting for page 17 OCR to complete...\n",
      "‚úì Page 17 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 17 saved: page_017.txt\n",
      "Preview: ‡•Ø. ‡§Ö‡§§‡§ø‡§ï‡§Æ ‡§µ‡§ø‡§ï‡§∏‡§ø‡§§ ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§≤‡§æ‡§à \"‡§µ‡§ø‡§∂‡•á‡§∑ ‡§§‡§•‡§æ ‡§™‡•É‡§•‡§ï ‡§µ‡•ç‡§Ø‡§µ‡§π‡§æ‡§∞...\n",
      "\n",
      "--- Processing page 18/31 ---\n",
      "‚úì Selected page 18\n",
      "‚úì Extract Page Text clicked for page 18 (attempt 1)\n",
      "‚è≥ Waiting for page 18 OCR to complete...\n",
      "‚úì Page 18 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 18 saved: page_018.txt\n",
      "Preview: ‡§∏‡§æ‡§Æ‡§æ‡§ú‡§ø‡§ï-‡§∏‡§æ‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø‡§ï ‡§∏‡§æ‡§Æ‡•Ä‡§™‡•ç‡§Ø‡§¨‡§æ‡§ü ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§Ö‡§µ‡§∏‡§∞‡§ï‡•ã ‡§Ö‡§ß‡§ø‡§ï‡§§‡§Æ...\n",
      "\n",
      "--- Processing page 19/31 ---\n",
      "‚úì Selected page 19\n",
      "‚úì Extract Page Text clicked for page 19 (attempt 1)\n",
      "‚è≥ Waiting for page 19 OCR to complete...\n",
      "‚úì Page 19 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 19 saved: page_019.txt\n",
      "Preview: ‡•´. ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§ø‡§Ø ‡§Ö‡§∞‡•ç‡§•‡§§‡§®‡•ç‡§§‡•ç‡§∞‡§≤‡§æ‡§à ‡§ü‡•á‡§µ‡§æ ‡§™‡•Å‡§ó‡•ç‡§®‡•á ‡§ó‡§∞‡•Ä ‡§∏‡•ç‡§µ‡§§‡§®‡•ç‡§§...\n",
      "\n",
      "--- Processing page 20/31 ---\n",
      "‚úì Selected page 20\n",
      "‚úì Extract Page Text clicked for page 20 (attempt 1)\n",
      "‚è≥ Waiting for page 20 OCR to complete...\n",
      "‚úì Page 20 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 20 saved: page_020.txt\n",
      "Preview: ‡•≠. ‡§≠‡§®‡•ç‡§∏‡§æ‡§∞‡§ú‡§®‡•ç‡§Ø, ‡§ó‡•à‡§∞‡§≠‡§®‡•ç‡§∏‡§æ‡§∞‡§ú‡§®‡•ç‡§Ø ‡§Ö‡§µ‡§∞‡•ã‡§ß ‡§∞ ‡§™‡§æ‡§∞‡§µ‡§π‡§®‡§ï‡§æ ‡§∏‡§Æ‡§∏‡•ç...\n",
      "\n",
      "--- Processing page 21/31 ---\n",
      "‚úì Selected page 21\n",
      "‚úì Extract Page Text clicked for page 21 (attempt 1)\n",
      "‚è≥ Waiting for page 21 OCR to complete...\n",
      "‚úì Page 21 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 21 saved: page_021.txt\n",
      "Preview: 3. ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§∂‡•ç‡§∞‡§Æ‡§ø‡§ï‡§ï‡•ã ‡§™‡•á‡§∂‡§æ‡§ó‡§§ ‡§π‡§ï‡§π‡§ø‡§§, ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ, ‡§∏‡§æ‡§Æ‡§æ‡§ú‡§ø‡§ï ...\n",
      "\n",
      "--- Processing page 22/31 ---\n",
      "‚úì Selected page 22\n",
      "‚úì Extract Page Text clicked for page 22 (attempt 1)\n",
      "‚è≥ Waiting for page 22 OCR to complete...\n",
      "‚úì Page 22 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 22 saved: page_022.txt\n",
      "Preview: ‡•≠. ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ‡§Æ‡§æ ‡§™‡§∞‡•á‡§ï‡§æ ‡§®‡•á‡§™‡§æ‡§≤‡•Ä‡§≤‡§æ‡§à ‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§™‡§∞‡§æ‡§Æ‡§∞‡•ç‡§∂, ‡§∞‡§æ‡§π‡§§ ‡§§...\n",
      "\n",
      "--- Processing page 23/31 ---\n",
      "‚úì Selected page 23\n",
      "‚úì Extract Page Text clicked for page 23 (attempt 1)\n",
      "‚è≥ Waiting for page 23 OCR to complete...\n",
      "‚úì Page 23 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 23 saved: page_023.txt\n",
      "Preview: ‡•®. ‡§∏‡§Æ‡§æ‡§µ‡•á‡§∂‡•Ä ‡§≤‡•ã‡§ï‡§§‡§æ‡§®‡•ç‡§§‡•ç‡§∞‡§ø‡§ï ‡§™‡•ç‡§∞‡§£‡§æ‡§≤‡•Ä ‡§§‡§•‡§æ ‡§µ‡§ø‡§ß‡§ø‡§ï‡•ã ‡§∂‡§æ‡§∏‡§® ‡§∞ ...\n",
      "\n",
      "--- Processing page 24/31 ---\n",
      "‚úì Selected page 24\n",
      "‚úì Extract Page Text clicked for page 24 (attempt 1)\n",
      "‚è≥ Waiting for page 24 OCR to complete...\n",
      "‚úì Page 24 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 24 saved: page_024.txt\n",
      "Preview: ‡§∞ ‡§§‡•ç‡§Ø‡§∏‡§¨‡§æ‡§ü ‡§Æ‡§æ‡§®‡§µ ‡§∏‡§≠‡•ç‡§Ø‡§§‡§æ‡§ï‡•ã ‡§µ‡§ø‡§ï‡§æ‡§∏‡§Æ‡§æ ‡§Ø‡•ã‡§ó‡§¶‡§æ‡§® ‡§™‡•Å‡§ó‡•á‡§ï‡•ã ‡§§‡§•‡•ç‡§Ø...\n",
      "\n",
      "--- Processing page 25/31 ---\n",
      "‚úì Selected page 25\n",
      "‚úì Extract Page Text clicked for page 25 (attempt 1)\n",
      "‚è≥ Waiting for page 25 OCR to complete...\n",
      "‚úì Page 25 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 25 saved: page_025.txt\n",
      "Preview: ‡•®. ‡§∏‡•Ç‡§ö‡§®‡§æ ‡§§‡§•‡§æ ‡§∏‡§û‡•ç‡§ö‡§æ‡§∞ ‡§™‡•ç‡§∞‡§µ‡§ø‡§ß‡§ø‡§ï‡•ã ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§Æ‡§æ‡§∞‡•ç‡§´‡§§ ‡§∏‡•á‡§µ‡§æ ‡§™...\n",
      "\n",
      "--- Processing page 26/31 ---\n",
      "‚úì Selected page 26\n",
      "‚úì Extract Page Text clicked for page 26 (attempt 1)\n",
      "‚è≥ Waiting for page 26 OCR to complete...\n",
      "‚úì Page 26 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 26 saved: page_026.txt\n",
      "Preview: ‡•™. ‡§™‡§∞‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§®‡•Ä‡§§‡§ø, ‡§Ö‡§®‡•ç‡§§‡§∞‡•ç‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§ø‡§Ø ‡§∏‡§Æ‡•ç‡§¨‡§®‡•ç‡§ß ‡§§‡§•‡§æ ‡§ï‡•Ç‡§ü...\n",
      "\n",
      "--- Processing page 27/31 ---\n",
      "‚úì Selected page 27\n",
      "‚úì Extract Page Text clicked for page 27 (attempt 1)\n",
      "‚è≥ Waiting for page 27 OCR to complete...\n",
      "‚úì Page 27 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 27 saved: page_027.txt\n",
      "Preview: ‡§∏‡§∞‡§ï‡§æ‡§∞‡•Ä ‡§®‡§ø‡§ï‡§æ‡§Ø‡§¨‡•Ä‡§ö ‡§Ö‡§≤‡§ó‡•ç‡§ó‡•à ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§∏‡§û‡•ç‡§ö‡§æ‡§∞ ‡§≤‡§æ‡§à‡§® (secur...\n",
      "\n",
      "--- Processing page 28/31 ---\n",
      "‚úì Selected page 28\n",
      "‚úì Extract Page Text clicked for page 28 (attempt 1)\n",
      "‚è≥ Waiting for page 28 OCR to complete...\n",
      "‚úì Page 28 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 28 saved: page_028.txt\n",
      "Preview: ‡•Ø.‡•®. ‡§Ø‡§∏ ‡§®‡•Ä‡§§‡§ø‡§ï‡•ã ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§®‡•ç‡§µ‡§Ø‡§®‡§ï‡•ã ‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂‡§®, ‡§∏‡•Å‡§™‡§∞‡§ø‡§µ‡•á‡§ï‡•ç‡§∑‡§£...\n",
      "\n",
      "--- Processing page 29/31 ---\n",
      "‚úì Selected page 29\n",
      "‚úì Extract Page Text clicked for page 29 (attempt 1)\n",
      "‚è≥ Waiting for page 29 OCR to complete...\n",
      "‚úì Page 29 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 29 saved: page_029.txt\n",
      "Preview: ‡•ß‡•¶. ‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ‡§É\n",
      "‡•ß‡•¶.‡•ß. ‡§Ø‡§∏ ‡§®‡•Ä‡§§‡§ø‡§ï‡•ã ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§®‡•ç‡§µ‡§Ø‡§®‡§ï‡•ã...\n",
      "\n",
      "--- Processing page 30/31 ---\n",
      "‚úì Selected page 30\n",
      "‚úì Extract Page Text clicked for page 30 (attempt 1)\n",
      "‚è≥ Waiting for page 30 OCR to complete...\n",
      "‚úì Page 30 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 30 saved: page_030.txt\n",
      "Preview: (‡§ñ)\n",
      "‡§≠‡•Ç‡§∞‡§æ‡§ú‡§®‡•Ä‡§§‡§ø‡§ï, ‡§∏‡§æ‡§Æ‡§∞‡§ø‡§ï, ‡§Ö‡§®‡•ç‡§§‡§∞‡•ç‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§ø‡§Ø ‡§∏‡§Æ‡•ç‡§¨‡§®‡•ç‡§ß ‡§∞ ...\n",
      "\n",
      "--- Processing page 31/31 ---\n",
      "‚úì Selected page 31\n",
      "‚úì Extract Page Text clicked for page 31 (attempt 1)\n",
      "‚è≥ Waiting for page 31 OCR to complete...\n",
      "‚úì Page 31 OCR completed\n",
      "‚úì Text extracted from ocrTextBox\n",
      "‚úÖ Page 31 saved: page_031.txt\n",
      "Preview: ‡§¶‡•á‡§ñ‡§æ ‡§™‡§∞‡•á‡§ï‡§æ ‡§®‡§µ‡•Ä‡§® ‡§™‡§æ‡§§‡•ç‡§∞, ‡§™‡•ç‡§∞‡§µ‡•É‡§§‡•ç‡§§‡§ø ‡§∞ ‡§™‡•ç‡§∞‡§µ‡§ø‡§ß‡§ø‡§≤‡•á ‡§≤‡•ç‡§Ø‡§æ‡§â...\n",
      "‚úÖ Chunk ‡§™‡§∞‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§®‡•Ä‡§§‡§ø ‡•®‡•¶‡•≠‡•≠ _chunk_001 COMPLETELY processed: 30/31 pages\n",
      "\n",
      "‚úÖ CHUNK 1 COMPLETED SUCCESSFULLY!\n",
      "   ‚Ä¢ Pages extracted: 30/31\n",
      "   ‚Ä¢ Output saved to: /var/home/ramrshrcg/Downloads/nepali datas folder/Nepali Materials/‡§™‡§∞‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§®‡•Ä‡§§‡§ø ‡•®‡•¶‡•≠‡•≠ _batch_ocr/extracted_text_‡§™‡§∞‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§®‡•Ä‡§§‡§ø ‡•®‡•¶‡•≠‡•≠ _chunk_001\n",
      "\n",
      "üîß Step 4: Cleaning up...\n",
      "\n",
      "üìä Step 5: Creating final summary...\n",
      "\n",
      "üéâ Batch processing complete!\n",
      "‚úÖ Successfully processed: 1/1 chunks\n",
      "üìÑ Total pages extracted: 30\n",
      "üìÅ Results saved in: /var/home/ramrshrcg/Downloads/nepali datas folder/Nepali Materials/‡§™‡§∞‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§®‡•Ä‡§§‡§ø ‡•®‡•¶‡•≠‡•≠ _batch_ocr\n",
      "\n",
      "üéâ BATCH PROCESSING COMPLETED! üéâ\n",
      "üìä Results Summary:\n",
      "   ‚Ä¢ Total chunks processed: 1/1\n",
      "   ‚Ä¢ Total pages extracted: 30\n",
      "   ‚Ä¢ Output directory: /var/home/ramrshrcg/Downloads/nepali datas folder/Nepali Materials/‡§™‡§∞‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§®‡•Ä‡§§‡§ø ‡•®‡•¶‡•≠‡•≠ _batch_ocr\n",
      "\n",
      "üìã Individual Chunk Results:\n",
      "   ‚úÖ Chunk 1: 30/31 pages\n"
     ]
    }
   ],
   "source": [
    "# USAGE EXAMPLE - Batch Process Large PDF\n",
    "# Replace the path below with your actual large PDF file path\n",
    "\n",
    "\n",
    "# Method 1: Split by file size (recommended for OCR website limits)\n",
    "print(\"üöÄ Starting batch processing with size-based splitting...\")\n",
    "try:\n",
    "    results = batch_process_pdf_chunks(\n",
    "        input_pdf_path=large_pdf_path,\n",
    "        max_size_mb=2 # 3MB chunks for optimal speed (faster processing)\n",
    "    )\n",
    "    \n",
    "    if results[\"success\"]:\n",
    "        print(f\"\\nüéâ BATCH PROCESSING COMPLETED! üéâ\")\n",
    "        print(f\"üìä Results Summary:\")\n",
    "        print(f\"   ‚Ä¢ Total chunks processed: {results['successful_chunks']}/{results['total_chunks']}\")\n",
    "        print(f\"   ‚Ä¢ Total pages extracted: {results['total_pages']}\")\n",
    "        print(f\"   ‚Ä¢ Output directory: {results['output_dir']}\")\n",
    "        \n",
    "        # Show individual chunk results\n",
    "        print(f\"\\nüìã Individual Chunk Results:\")\n",
    "        for i, chunk_result in enumerate(results['chunk_results'], 1):\n",
    "            if chunk_result[\"success\"]:\n",
    "                print(f\"   ‚úÖ Chunk {i}: {chunk_result['extracted_pages']}/{chunk_result['total_pages']} pages\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Chunk {i}: Failed - {chunk_result.get('error', 'Unknown error')}\")\n",
    "    else:\n",
    "        print(\"‚ùå Batch processing failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in batch processing: {e}\")\n",
    "\n",
    "# Uncomment the lines below if you want to use page-based splitting instead:\n",
    "# print(\"üöÄ Starting batch processing with page-based splitting...\")\n",
    "# results = batch_process_pdf_chunks(\n",
    "#     input_pdf_path=large_pdf_path,\n",
    "#     pages_per_chunk=35  # 35 pages per chunk for optimal speed\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10b3c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c286c9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Improved page processing function ready!\n",
      "üîß This function fixes:\n",
      "   ‚Ä¢ Page number mismatch issues\n",
      "   ‚Ä¢ Duplicate content detection\n",
      "   ‚Ä¢ Better content verification\n",
      "   ‚Ä¢ Detailed error tracking\n"
     ]
    }
   ],
   "source": [
    "# IMPROVED PAGE PROCESSING - FIXES PAGE NUMBER MISMATCH\n",
    "\n",
    "def process_pages_with_verification(driver, chunk_output_dir, chunk_name, total_pages):\n",
    "    \"\"\"\n",
    "    Improved page processing that fixes page number mismatch issues\n",
    "    \n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance\n",
    "        chunk_output_dir: Output directory for this chunk\n",
    "        chunk_name: Name of the chunk being processed\n",
    "        total_pages: Total number of pages to process\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with processing results\n",
    "    \"\"\"\n",
    "    extracted_pages = 0\n",
    "    error_pages = []\n",
    "    duplicate_content_detected = []\n",
    "    \n",
    "    # Store previous page content to detect duplicates\n",
    "    previous_content_hash = None\n",
    "    \n",
    "    print(f\"üîç Starting improved page processing for {total_pages} pages...\")\n",
    "    \n",
    "    for page_num in range(1, total_pages + 1):\n",
    "        try:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"üìÑ Processing Page {page_num}/{total_pages}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            # STEP 1: Clear any existing content in textbox\n",
    "            try:\n",
    "                ocr_textbox = driver.find_element(By.ID, 'ocrTextBox')\n",
    "                # Clear the textbox using JavaScript\n",
    "                driver.execute_script(\"arguments[0].value = ''; arguments[0].innerHTML = '';\", ocr_textbox)\n",
    "                # Also try clearing with standard clear method\n",
    "                ocr_textbox.clear()\n",
    "                print(f\"üßπ Cleared textbox for page {page_num}\")\n",
    "                time.sleep(2)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö† Could not clear textbox: {e}\")\n",
    "            \n",
    "            # STEP 2: Click on the specific page thumbnail\n",
    "            page_clicked = False\n",
    "            for attempt in range(3):  # Try up to 3 times\n",
    "                try:\n",
    "                    page_element = driver.find_element(By.ID, f'page_{page_num}')\n",
    "                    \n",
    "                    # Scroll to the page element if needed\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView(true);\", page_element)\n",
    "                    time.sleep(1)\n",
    "                    \n",
    "                    # Click the page\n",
    "                    page_element.click()\n",
    "                    print(f\"‚úì Clicked on page {page_num} thumbnail (attempt {attempt + 1})\")\n",
    "                    page_clicked = True\n",
    "                    \n",
    "                    # Wait for selection to register\n",
    "                    time.sleep(3)\n",
    "                    break\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö† Failed to click page {page_num} (attempt {attempt + 1}): {e}\")\n",
    "                    time.sleep(2)\n",
    "            \n",
    "            if not page_clicked:\n",
    "                print(f\"‚ùå Could not select page {page_num}\")\n",
    "                error_pages.append(page_num)\n",
    "                continue\n",
    "            \n",
    "            # STEP 3: Verify page selection (look for visual indicators)\n",
    "            page_selected = False\n",
    "            try:\n",
    "                # Check if page has selection styling\n",
    "                selected_page = driver.find_element(By.CSS_SELECTOR, \n",
    "                    f'img[id=\"page_{page_num}\"][class*=\"selected\"], ' +\n",
    "                    f'img[id=\"page_{page_num}\"][style*=\"border\"], ' +\n",
    "                    f'img[id=\"page_{page_num}\"][style*=\"outline\"]')\n",
    "                print(f\"‚úì Page {page_num} selection visually confirmed\")\n",
    "                page_selected = True\n",
    "            except:\n",
    "                print(f\"‚ö† Page {page_num} selection not visually confirmed, but proceeding...\")\n",
    "                page_selected = True  # Assume it worked\n",
    "            \n",
    "            # STEP 4: Click \"Extract Page Text\" button\n",
    "            extract_clicked = False\n",
    "            for attempt in range(3):\n",
    "                try:\n",
    "                    # Wait for any processing from page selection to complete\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                    extract_button = driver.find_element(By.XPATH, \n",
    "                        \"//button[contains(text(), 'Extract Page Text')] | \" +\n",
    "                        \"//input[contains(@value, 'Extract Page Text')] | \" +\n",
    "                        \"//button[contains(@class, 'extract')] | \" +\n",
    "                        \"//input[contains(@class, 'extract')]\")\n",
    "                    \n",
    "                    extract_button.click()\n",
    "                    print(f\"‚úì Extract Page Text clicked for page {page_num} (attempt {attempt + 1})\")\n",
    "                    extract_clicked = True\n",
    "                    break\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö† Extract button not found for page {page_num} (attempt {attempt + 1}): {e}\")\n",
    "                    time.sleep(3)\n",
    "            \n",
    "            if not extract_clicked:\n",
    "                print(f\"‚ùå Could not click Extract Text for page {page_num}\")\n",
    "                error_pages.append(page_num)\n",
    "                continue\n",
    "            \n",
    "            # STEP 5: Wait for OCR processing with smart timeout\n",
    "            print(f\"‚è≥ Waiting for page {page_num} OCR processing...\")\n",
    "            \n",
    "            # Initial wait for processing to start\n",
    "            time.sleep(8)\n",
    "            \n",
    "            # Wait for content to appear with verification\n",
    "            max_wait = 90\n",
    "            waited = 0\n",
    "            content_ready = False\n",
    "            \n",
    "            while waited < max_wait and not content_ready:\n",
    "                try:\n",
    "                    ocr_textbox = driver.find_element(By.ID, 'ocrTextBox')\n",
    "                    current_content = ocr_textbox.get_attribute('value') or ocr_textbox.text or \"\"\n",
    "                    \n",
    "                    # Check if we have meaningful content\n",
    "                    if current_content and len(current_content.strip()) > 10:\n",
    "                        # Additional check: make sure it's not the same as previous page\n",
    "                        import hashlib\n",
    "                        current_hash = hashlib.md5(current_content.strip().encode()).hexdigest()\n",
    "                        \n",
    "                        if previous_content_hash and current_hash == previous_content_hash:\n",
    "                            print(f\"‚ö† Same content as previous page detected, waiting more...\")\n",
    "                            time.sleep(5)\n",
    "                            waited += 5\n",
    "                            continue\n",
    "                        else:\n",
    "                            content_ready = True\n",
    "                            print(f\"‚úì Page {page_num} content ready ({len(current_content)} chars)\")\n",
    "                            break\n",
    "                    else:\n",
    "                        time.sleep(5)\n",
    "                        waited += 5\n",
    "                        if waited % 15 == 0:  # Print status every 15 seconds\n",
    "                            print(f\"‚è≥ Still waiting for page {page_num} content... ({waited}s)\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    time.sleep(5)\n",
    "                    waited += 5\n",
    "            \n",
    "            # STEP 6: Extract the text content\n",
    "            extracted_text = None\n",
    "            extraction_method = None\n",
    "            \n",
    "            # Method 1: Direct textbox extraction\n",
    "            try:\n",
    "                ocr_textbox = driver.find_element(By.ID, 'ocrTextBox')\n",
    "                extracted_text = ocr_textbox.get_attribute('value') or ocr_textbox.text\n",
    "                \n",
    "                if extracted_text and len(extracted_text.strip()) > 5:\n",
    "                    extraction_method = \"textbox\"\n",
    "                    print(f\"‚úì Text extracted via textbox ({len(extracted_text)} chars)\")\n",
    "                else:\n",
    "                    extracted_text = None\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö† Textbox extraction failed: {e}\")\n",
    "            \n",
    "            # Method 2: Copy button extraction\n",
    "            if not extracted_text:\n",
    "                try:\n",
    "                    copy_button = driver.find_element(By.XPATH, \n",
    "                        \"//button[contains(text(), 'Copy')] | \" +\n",
    "                        \"//input[contains(@value, 'Copy')]\")\n",
    "                    copy_button.click()\n",
    "                    time.sleep(3)\n",
    "                    \n",
    "                    import pyperclip\n",
    "                    extracted_text = pyperclip.paste()\n",
    "                    \n",
    "                    if extracted_text and len(extracted_text.strip()) > 5:\n",
    "                        extraction_method = \"clipboard\"\n",
    "                        print(f\"‚úì Text extracted via clipboard ({len(extracted_text)} chars)\")\n",
    "                    else:\n",
    "                        extracted_text = None\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö† Clipboard extraction failed: {e}\")\n",
    "            \n",
    "            # STEP 7: Content validation and duplicate detection\n",
    "            if extracted_text and extracted_text.strip():\n",
    "                # Check for duplicate content\n",
    "                import hashlib\n",
    "                current_hash = hashlib.md5(extracted_text.strip().encode()).hexdigest()\n",
    "                \n",
    "                if previous_content_hash and current_hash == previous_content_hash:\n",
    "                    print(f\"üö® DUPLICATE CONTENT DETECTED for page {page_num}!\")\n",
    "                    duplicate_content_detected.append(page_num)\n",
    "                    \n",
    "                    # Still save it but mark it clearly\n",
    "                    page_filename = f\"page_{page_num:03d}_DUPLICATE.txt\"\n",
    "                else:\n",
    "                    page_filename = f\"page_{page_num:03d}.txt\"\n",
    "                \n",
    "                previous_content_hash = current_hash\n",
    "            else:\n",
    "                page_filename = f\"page_{page_num:03d}_empty.txt\"\n",
    "            \n",
    "            # STEP 8: Save the content\n",
    "            if extracted_text and extracted_text.strip():\n",
    "                page_filepath = os.path.join(chunk_output_dir, page_filename)\n",
    "                \n",
    "                with open(page_filepath, 'w', encoding='utf-8') as f:\n",
    "                    f.write(f\"Chunk: {chunk_name}\\\\n\")\n",
    "                    f.write(f\"Page: {page_num}\\\\n\")\n",
    "                    f.write(f\"Extraction Method: {extraction_method}\\\\n\")\n",
    "                    f.write(f\"Content Length: {len(extracted_text)} characters\\\\n\")\n",
    "                    f.write(f\"Content Hash: {current_hash[:8]}\\\\n\")\n",
    "                    f.write(f\"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\\\\n\")\n",
    "                    \n",
    "                    if page_num in duplicate_content_detected:\n",
    "                        f.write(f\"*** WARNING: DUPLICATE CONTENT DETECTED ***\\\\n\")\n",
    "                    \n",
    "                    f.write(\"=\"*60 + \"\\\\n\")\n",
    "                    f.write(extracted_text)\n",
    "                    f.write(\"\\\\n\" + \"=\"*60 + \"\\\\n\")\n",
    "                \n",
    "                print(f\"‚úÖ Page {page_num} saved: {page_filename}\")\n",
    "                \n",
    "                # Show preview without newlines\n",
    "                preview = extracted_text[:100].replace('\\\\n', ' ').replace('\\\\r', ' ')\n",
    "                print(f\"üìÑ Preview: {preview}...\")\n",
    "                \n",
    "                extracted_pages += 1\n",
    "            else:\n",
    "                # Save empty file with debugging info\n",
    "                page_filepath = os.path.join(chunk_output_dir, page_filename)\n",
    "                \n",
    "                with open(page_filepath, 'w', encoding='utf-8') as f:\n",
    "                    f.write(f\"Chunk: {chunk_name}\\\\n\")\n",
    "                    f.write(f\"Page: {page_num}\\\\n\")\n",
    "                    f.write(f\"Status: No content extracted\\\\n\")\n",
    "                    f.write(f\"Page Selected: {page_selected}\\\\n\")\n",
    "                    f.write(f\"Extract Clicked: {extract_clicked}\\\\n\")\n",
    "                    f.write(f\"Content Ready: {content_ready}\\\\n\")\n",
    "                    f.write(f\"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\\\\n\")\n",
    "                \n",
    "                print(f\"‚ö† Page {page_num} - No content extracted\")\n",
    "            \n",
    "            # Brief pause before next page\n",
    "            time.sleep(3)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Critical error processing page {page_num}: {e}\")\n",
    "            error_pages.append(page_num)\n",
    "            continue\n",
    "    \n",
    "    # Create detailed summary\n",
    "    summary_path = os.path.join(chunk_output_dir, \"processing_summary.txt\")\n",
    "    with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"Improved Page Processing Summary\\\\n\")\n",
    "        f.write(f\"=\" * 50 + \"\\\\n\")\n",
    "        f.write(f\"Chunk: {chunk_name}\\\\n\")\n",
    "        f.write(f\"Total Pages: {total_pages}\\\\n\")\n",
    "        f.write(f\"Successfully Extracted: {extracted_pages}\\\\n\")\n",
    "        f.write(f\"Failed Pages: {len(error_pages)}\\\\n\")\n",
    "        f.write(f\"Duplicate Content Detected: {len(duplicate_content_detected)}\\\\n\")\n",
    "        f.write(f\"Processing Time: {time.strftime('%Y-%m-%d %H:%M:%S')}\\\\n\\\\n\")\n",
    "        \n",
    "        if error_pages:\n",
    "            f.write(f\"Failed Pages: {error_pages}\\\\n\\\\n\")\n",
    "        if duplicate_content_detected:\n",
    "            f.write(f\"Pages with Duplicate Content: {duplicate_content_detected}\\\\n\\\\n\")\n",
    "    \n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"üìä Processing Complete for Chunk: {chunk_name}\")\n",
    "    print(f\"‚úÖ Successfully extracted: {extracted_pages}/{total_pages} pages\")\n",
    "    if error_pages:\n",
    "        print(f\"‚ùå Failed pages: {error_pages}\")\n",
    "    if duplicate_content_detected:\n",
    "        print(f\"üö® Duplicate content detected on pages: {duplicate_content_detected}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return {\n",
    "        \"extracted_pages\": extracted_pages,\n",
    "        \"error_pages\": error_pages,\n",
    "        \"duplicate_pages\": duplicate_content_detected,\n",
    "        \"total_pages\": total_pages\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Improved page processing function ready!\")\n",
    "print(\"üîß This function fixes:\")\n",
    "print(\"   ‚Ä¢ Page number mismatch issues\")\n",
    "print(\"   ‚Ä¢ Duplicate content detection\") \n",
    "print(\"   ‚Ä¢ Better content verification\")\n",
    "print(\"   ‚Ä¢ Detailed error tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98c5376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # USAGE EXAMPLE - Using the Improved Page Processing\n",
    "\n",
    "# def process_single_chunk_improved(chunk_path, output_dir):\n",
    "#     \"\"\"\n",
    "#     Process a single PDF chunk using the improved page processing method\n",
    "    \n",
    "#     Args:\n",
    "#         chunk_path: Path to the PDF chunk file\n",
    "#         output_dir: Output directory for extracted text\n",
    "    \n",
    "#     Returns:\n",
    "#         Processing results dictionary\n",
    "#     \"\"\"\n",
    "#     print(f\"üöÄ Starting improved processing for: {os.path.basename(chunk_path)}\")\n",
    "    \n",
    "#     # Initialize browser\n",
    "#     driver = webdriver.Firefox()\n",
    "    \n",
    "#     try:\n",
    "#         chunk_filename = os.path.basename(chunk_path)\n",
    "#         chunk_name = os.path.splitext(chunk_filename)[0]\n",
    "        \n",
    "#         # Create output directory for this chunk\n",
    "#         chunk_output_dir = os.path.join(output_dir, f\"extracted_text_{chunk_name}\")\n",
    "#         if not os.path.exists(chunk_output_dir):\n",
    "#             os.makedirs(chunk_output_dir)\n",
    "        \n",
    "#         # Navigate and upload file\n",
    "#         print(\"üåê Loading OCR website...\")\n",
    "#         driver.get('https://www.i2ocr.com/free-online-nepali-ocr')\n",
    "#         time.sleep(3)\n",
    "        \n",
    "#         # Upload file\n",
    "#         print(\"üìÑ Uploading file...\")\n",
    "#         upload_button = driver.find_element(By.ID, 'i2ocr_uploadedfile')\n",
    "#         upload_button.send_keys(chunk_path)\n",
    "#         time.sleep(3)\n",
    "        \n",
    "#         # Manual intervention point\n",
    "#         print(\"\\\\n‚ö†Ô∏è  MANUAL STEPS REQUIRED:\")\n",
    "#         print(\"üë§ Please complete the following in the browser:\")\n",
    "#         print(\"   1. Handle any cookie dialogs\")\n",
    "#         print(\"   2. Solve any CAPTCHA challenges\")\n",
    "#         print(\"   3. Click 'Extract Text' button\")\n",
    "#         print(\"   4. Wait for initial processing to complete\")\n",
    "        \n",
    "#         input(\"\\\\n‚úã Press Enter when ready to start page-by-page extraction...\")\n",
    "        \n",
    "#         # Wait for page viewer to load\n",
    "#         print(\"‚è≥ Waiting for page viewer...\")\n",
    "#         time.sleep(10)\n",
    "        \n",
    "#         # Find total pages\n",
    "#         page_elements = driver.find_elements(By.CSS_SELECTOR, 'img[id^=\"page_\"]')\n",
    "#         total_pages = len(page_elements)\n",
    "#         print(f\"üìÑ Found {total_pages} pages to process\")\n",
    "        \n",
    "#         if total_pages == 0:\n",
    "#             print(\"‚ùå No pages found!\")\n",
    "#             return {\"success\": False, \"error\": \"No pages found\"}\n",
    "        \n",
    "#         # Use the improved page processing\n",
    "#         results = process_pages_with_verification(driver, chunk_output_dir, chunk_name, total_pages)\n",
    "        \n",
    "#         # Show final results\n",
    "#         print(f\"\\\\nüéâ Processing Complete!\")\n",
    "#         print(f\"üìä Results:\")\n",
    "#         print(f\"   ‚Ä¢ Total pages: {results['total_pages']}\")\n",
    "#         print(f\"   ‚Ä¢ Successfully extracted: {results['extracted_pages']}\")\n",
    "#         print(f\"   ‚Ä¢ Failed pages: {len(results['error_pages'])}\")\n",
    "#         print(f\"   ‚Ä¢ Duplicate content pages: {len(results['duplicate_pages'])}\")\n",
    "#         print(f\"   ‚Ä¢ Output directory: {chunk_output_dir}\")\n",
    "        \n",
    "#         if results['error_pages']:\n",
    "#             print(f\"‚ùå Failed pages: {results['error_pages']}\")\n",
    "#         if results['duplicate_pages']:\n",
    "#             print(f\"üö® Pages with duplicate content: {results['duplicate_pages']}\")\n",
    "#             print(\"   üìù These pages are marked as '_DUPLICATE' in filenames\")\n",
    "        \n",
    "#         return {\n",
    "#             \"success\": True,\n",
    "#             \"chunk_name\": chunk_name,\n",
    "#             \"results\": results,\n",
    "#             \"output_dir\": chunk_output_dir\n",
    "#         }\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error in processing: {e}\")\n",
    "#         return {\"success\": False, \"error\": str(e)}\n",
    "    \n",
    "#     finally:\n",
    "#         # Close browser\n",
    "#         print(\"\\\\nüîß Closing browser...\")\n",
    "#         driver.quit()\n",
    "\n",
    "# # Example usage:\n",
    "# # chunk_file = \"/path/to/your/chunk.pdf\"\n",
    "# # output_directory = \"/path/to/output\"\n",
    "# # result = process_single_chunk_improved(chunk_file, output_directory)\n",
    "\n",
    "# print(\"\\\\n‚úÖ Improved processing function ready to use!\")\n",
    "# print(\"üéØ Key improvements:\")\n",
    "# print(\"   ‚Ä¢ Clears textbox before each page\")\n",
    "# print(\"   ‚Ä¢ Verifies page selection\")\n",
    "# print(\"   ‚Ä¢ Detects duplicate content\")\n",
    "# print(\"   ‚Ä¢ Better error handling\")\n",
    "# print(\"   ‚Ä¢ Content hash verification\")\n",
    "# print(\"   ‚Ä¢ Detailed processing logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83411d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
